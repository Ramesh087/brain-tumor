{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c7010f",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'myenv (Python 3.10.6)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '\"d:/brain tumor/myenv/Scripts/python.exe\" -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40a9f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71afdf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from PIL import Image\n",
    "from numpy import asarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3071a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirct ='D:\\brain tumor'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a7c3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = os.listdir(dirct+'/Brain Tumor Data')\n",
    "classes = {'no_tumor':0, 'pituitary_tumor':1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae456ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_tumor_frompath= dirct+'/Brain Tumor Data/Training/no_tumor'\n",
    "no_tumor_topath = dirct+'/Brain Tumor Data/Agumented_Data/Training/no_tumor/'\n",
    "no_tumor_images=glob(no_tumor_frompath+'/*')\n",
    "c=0\n",
    "for path in no_tumor_images:\n",
    "  img=Image.open(path)\n",
    "  img=img.resize((224,224))\n",
    "  img.save(no_tumor_topath+str(c)+'.jpg')\n",
    "  c+=1\n",
    "  mirimg=img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "  mirimg.save(no_tumor_topath+str(c)+'.jpg')\n",
    "  c+=1\n",
    "  rotimg=img.rotate(90,Image.NEAREST, expand = 1)\n",
    "  rotimg.save(no_tumor_topath+str(c)+'.jpg')\n",
    "  c+=1\n",
    "  mirrotimg=rotimg.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "  mirrotimg.save(no_tumor_topath+str(c)+'.jpg')\n",
    "  c+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386c29d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "frompath = dirct+'Brain Tumor Data/Training/pituitary_tumor'\n",
    "topath = dirct+'Brain Tumor Data/Agumented_Data/Training/pituitary_tumor/'\n",
    "images=glob(frompath+'/*')\n",
    "c=0\n",
    "for path in images:\n",
    "  img=Image.open(path)\n",
    "  img=img.resize((224,224))\n",
    "  img.save(topath+str(c)+'.jpg')\n",
    "  c+=1\n",
    "  mirimg=img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "  mirimg.save(topath+str(c)+'.jpg')\n",
    "  c+=1\n",
    "  rotimg=img.rotate(90,Image.NEAREST, expand = 1)\n",
    "  rotimg.save(topath+str(c)+'.jpg')\n",
    "  c+=1\n",
    "  mirrotimg=rotimg.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "  mirrotimg.save(topath+str(c)+'.jpg')\n",
    "  c+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf4990b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "X=[]\n",
    "Y=[]\n",
    "for cls in classes:\n",
    "  pth = dirct+'/Brain Tumor Data/Agumented_Data/Training/'+cls\n",
    "  for j in os.listdir(pth):\n",
    "    img = cv2.imread(pth+'/'+j)   \n",
    "    # img = cv2.resize(img,dsize=(224,224))\n",
    "    X.append(img)\n",
    "    Y.append(classes[cls])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f33ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X[0],cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796c4237",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(X,Y,random_state=10,test_size=0.20)\n",
    "print (\"Shape of an image in x_train: \", x_train[0].shape)\n",
    "print (\"Shape of an image in x_test: \", x_test[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958606fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "le = preprocessing.LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.fit_transform(y_test)\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=2)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=2)\n",
    "y_train = np.array(y_train)\n",
    "x_train = np.array(x_train)\n",
    "y_test = np.array(y_test)\n",
    "x_test = np.array(x_test) \n",
    "print(\"x_train Shape: \", x_train.shape) \n",
    "print(\"x_test Shape: \", x_test.shape)\n",
    "print(\"y_train Shape: \", y_train.shape) \n",
    "print(\"y_test Shape: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416ae972",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50, VGG16\n",
    "from tensorflow.keras.layers import Dense, Flatten, Concatenate, Dropout, Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Define the input shape\n",
    "input_shape = (224, 224, 3)  # Adjust as needed\n",
    "inputs = Input(shape=input_shape)\n",
    "\n",
    "# Load pre-trained ResNet50 and VGG16 models\n",
    "resnet = ResNet50(weights='imagenet', include_top=False, input_tensor=inputs)\n",
    "vgg16 = VGG16(weights='imagenet', include_top=False, input_tensor=inputs)\n",
    "\n",
    "# Freeze the base models\n",
    "for layer in resnet.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "for layer in vgg16.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Flatten outputs from both models\n",
    "resnet_output = Flatten()(resnet.output)\n",
    "vgg16_output = Flatten()(vgg16.output)\n",
    "\n",
    "# Concatenate features from both models\n",
    "merged = Concatenate()([resnet_output, vgg16_output])\n",
    "\n",
    "# Add fully connected layers\n",
    "x = Dense(512, activation='relu')(merged)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "# Output layer\n",
    "outputs = Dense(2, activation='softmax')(x)  # Adjust the number of classes as needed\n",
    "\n",
    "# Create the hybrid model\n",
    "hybrid_model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile the model\n",
    "hybrid_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Summary of the hybrid model\n",
    "hybrid_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86aae42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_keras_picklable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7871d8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "\n",
    "def make_keras_picklable():\n",
    "    def __getstate__(self):\n",
    "        model_str = \"\"\n",
    "        with tempfile.NamedTemporaryFile(suffix='.hdf5', delete=True) as fd:\n",
    "            save_model(self, fd.name, overwrite=True)\n",
    "            model_str = fd.read()\n",
    "        d = {'model_str': model_str}\n",
    "        return d\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        with tempfile.NamedTemporaryFile(suffix='.hdf5', delete=True) as fd:\n",
    "            fd.write(state['model_str'])\n",
    "            fd.flush()\n",
    "            model = load_model(fd.name)\n",
    "        self.__dict__ = model.__dict__\n",
    "\n",
    "\n",
    "    cls = Model\n",
    "    cls.__getstate__ = __getstate__\n",
    "    cls.__setstate__ = __setstate__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9ddda4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename= 'brain_tumor_model.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dc7a4bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n",
      "...layers\n",
      "......conv2d\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_10\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_11\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_12\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_2\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_3\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_4\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_5\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_6\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_7\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_8\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_9\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense_2\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense_3\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......global_average_pooling2d\n",
      ".........vars\n",
      "......input_layer\n",
      ".........vars\n",
      "......max_pooling2d\n",
      ".........vars\n",
      "......max_pooling2d_1\n",
      ".........vars\n",
      "......max_pooling2d_2\n",
      ".........vars\n",
      "......max_pooling2d_3\n",
      ".........vars\n",
      "......max_pooling2d_4\n",
      ".........vars\n",
      "...metrics\n",
      "......mean\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......mean_metric_wrapper\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "...optimizer\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      ".........10\n",
      ".........11\n",
      ".........12\n",
      ".........13\n",
      ".........14\n",
      ".........15\n",
      ".........16\n",
      ".........2\n",
      ".........3\n",
      ".........4\n",
      ".........5\n",
      ".........6\n",
      ".........7\n",
      ".........8\n",
      ".........9\n",
      "...vars\n",
      "Keras model archive saving:\n",
      "File Name                                             Modified             Size\n",
      "variables.h5                                   2023-04-01 12:20:56     84145360\n",
      "config.json                                    2023-04-01 12:20:56        12542\n",
      "metadata.json                                  2023-04-01 12:20:56           64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pickle.dump(model,open(filename,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "832c931f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-21 21:20:40.564385: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-21 21:20:40.671616: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-21 21:20:40.671637: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-21 21:20:41.160193: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-21 21:20:41.160243: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-21 21:20:41.160247: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras model archive loading:\n",
      "File Name                                             Modified             Size\n",
      "variables.h5                                   2023-03-20 22:41:38     84145360\n",
      "config.json                                    2023-03-20 22:41:36        12542\n",
      "metadata.json                                  2023-03-20 22:41:36           64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-21 21:20:42.250624: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-03-21 21:20:42.250650: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-03-21 21:20:42.250667: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (rishav): /proc/driver/nvidia/version does not exist\n",
      "2023-03-21 21:20:42.250832: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r)>) loading:\n",
      "...layers\n",
      "......conv2d\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_10\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_11\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_12\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_2\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_3\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_4\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_5\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_6\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_7\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_8\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_9\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense_2\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense_3\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......global_average_pooling2d\n",
      ".........vars\n",
      "......input_layer\n",
      ".........vars\n",
      "......max_pooling2d\n",
      ".........vars\n",
      "......max_pooling2d_1\n",
      ".........vars\n",
      "......max_pooling2d_2\n",
      ".........vars\n",
      "......max_pooling2d_3\n",
      ".........vars\n",
      "......max_pooling2d_4\n",
      ".........vars\n",
      "...metrics\n",
      "......mean\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......mean_metric_wrapper\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "...optimizer\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      ".........10\n",
      ".........11\n",
      ".........12\n",
      ".........13\n",
      ".........14\n",
      ".........15\n",
      ".........16\n",
      ".........2\n",
      ".........3\n",
      ".........4\n",
      ".........5\n",
      ".........6\n",
      ".........7\n",
      ".........8\n",
      ".........9\n",
      "...vars\n"
     ]
    }
   ],
   "source": [
    "load_model = pickle.load(open(filename,'rb'))\n",
    "#load_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eb10b79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP for Pima Indians Dataset Serialize to JSON and HDF5\n",
    "from tensorflow.keras.models import model_from_json\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ea644bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "34a350c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 186s 6s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[9.9999994e-01, 4.7917198e-10],\n",
       "       [8.7774271e-07, 9.9999911e-01],\n",
       "       [9.9999005e-01, 9.8449254e-06],\n",
       "       ...,\n",
       "       [7.0458255e-04, 9.9929541e-01],\n",
       "       [1.3789128e-10, 1.0000000e+00],\n",
       "       [5.6625770e-05, 9.9994338e-01]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "08146434",
   "metadata": {},
   "outputs": [],
   "source": [
    "dec= {0: 'No Tumor', 1:'Positive Tumor'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1b7edea1",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/media/rishav/Local Disk/Personal/MachineLearningProjects/Brain-Tumor-Detection/Brain Tumor Data/Training/pituitary_tumor/p (99).jpgBrain Tumor Data/Training/pituitary_tumor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_18880/1707451006.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m18\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'Brain Tumor Data/Training/pituitary_tumor'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpredict_img\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'Brain Tumor Data/Training/pituitary_tumor/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/media/rishav/Local Disk/Personal/MachineLearningProjects/Brain-Tumor-Detection/Brain Tumor Data/Training/pituitary_tumor/p (99).jpgBrain Tumor Data/Training/pituitary_tumor'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x1200 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(18,12))\n",
    "p=os.listdir(path+'Brain Tumor Data/Training/pituitary_tumor')\n",
    "c=1\n",
    "predict_img=[]\n",
    "for i in os.listdir(path+'Brain Tumor Data/Training/pituitary_tumor/')[:16]:\n",
    "  plt.subplot(4,4,c)\n",
    "\n",
    "  img=cv2.imread(path+'Brain Tumor Data/Training/pituitary_tumor/'+i)\n",
    "  img1 = cv2.resize(img,(224,224))\n",
    "  predict_img.append(img1)\n",
    "  p= load_model.predict(np.array(predict_img))\n",
    "  result = np.argmax(p, axis =1)\n",
    "  plt.title(dec[result[c-1]])\n",
    "  plt.imshow(img,cmap='gray')\n",
    "  plt.axis('off')\n",
    "  c+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c3d6408",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16159/3463644309.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m18\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/rishav/Brain Tumor Detection/Brain Tumor Data/Training/no_tumor'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpredict_img\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/rishav/Brain Tumor Detection/Brain Tumor Data/Training/no_tumor/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(18,12))\n",
    "p=os.listdir('/home/rishav/Brain Tumor Detection/Brain Tumor Data/Training/no_tumor')\n",
    "c=1\n",
    "predict_img=[]\n",
    "for i in os.listdir('/home/rishav/Brain Tumor Detection/Brain Tumor Data/Training/no_tumor/')[:16]:\n",
    "  plt.subplot(4,4,c)\n",
    "\n",
    "  img=cv2.imread('/home/rishav/Brain Tumor Detection/Brain Tumor Data/Training/no_tumor/'+i)\n",
    "  img1 = cv2.resize(img,(224,224))\n",
    "  predict_img.append(img1)\n",
    "  p= model.predict(np.array(predict_img))\n",
    "  result = np.argmax(p, axis =1)\n",
    "  plt.title(dec[result[c-1]])\n",
    "  plt.imshow(img,cmap='gray')\n",
    "  plt.axis('off')\n",
    "  c+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
